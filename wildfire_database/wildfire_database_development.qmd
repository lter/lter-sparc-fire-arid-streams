---
title: "wildfire database development"
author: "S. Earl"
format: gfm
---

# overview ---------------------------------------------------------------------

Documentation and general workflow overview for developing the wildfire database to support the CRASS 2.0 study that will investigate relationships between wildfire and aquatic biogeochemistry in the arid western United States. The focus here is strictly on development of the wildfire database that stores information such as the geometry of study catchments, discharge, and water chemistry data. Other workflows for harvesting, for example covariates, are detailed in other files within this repository. The workflow draws heavily on the [firearea](https://srearl.gitlab.io/firearea/index.html) package that also support this effort. Important is that this is not a reproducible workflow: there is a time-sensitive component to some of the `firearea` queries, especially those related to discharge, such that there could be different returns based on the date of query, and this workflow features iterative steps, such as querying data for USGS sites in individual sites but for multiple states that is not shown, and some, particularly, database actions are run once after initial table construction but not when data are added subsequently.

This workflow documents initial database implementation that featured data from the four-corner states, California, and Nevada. For each of these six states, the water chemistry was addressed (i.e., pulled from the web and added to the database) state-by-state but data in subsequent steps (e.g., discharge, catchments) were addressed by pulling seed data from all six states that had been added to the database initially.

The number of sites pares quickly based on data availability. Using data from the six states listed above, there were 4941 unique USGS sites in the water_chem table, with the number of unique sites falling to 1508 in the discharge_daily table, and to 1423 in the catchments table. The large paring from the water_chem to discharge_daily tables appears due to availability of discharge data. While having discharge data was part of the conditions established in firearea::identify_sites_to_delineate, having but just a few or even one discharge data point would satisfy that condition.


# database and schema ----------------------------------------------------------

```{r}
#| eval: FALSE

DBI::dbExecute(
  conn      = pg,
  statement = "
  CREATE DATABASE wildfire 
    WITH 
    ENCODING = 'UTF8'
  ;
  "
)

DBI::dbExecute(pg, "CREATE SCHEMA IF NOT EXISTS firearea AUTHORIZATION _user_ ;")
```


# water chemistry --------------------------------------------------------------

For each state of interest, here California, identify USGS sites that fit a minimum set of criteria (see [firearea documentation](https://srearl.gitlab.io/firearea/reference/identify_sites_to_delineate.html)) for more details and harvest targeted water chemistry data for these sites. These data are loaded into the `water_chem` table of the firearea schema.

This is a good example of where this workflow is not reproducible (in the sense of sourcing this document) as this procedure is addressed state-by-state; we perform these steps iteratively for each state of interest.

```{r}
#| eval: FALSE

stream_sites_candidates <- firearea::identify_sites_to_delineate(state_code = "CA") |>
  dplyr::mutate(usgs_site = paste0("USGS-", site_no))

usgs_water_chem <- split(
  x = stream_sites_candidates,
  f = stream_sites_candidates$usgs_site
) |>
  {\(site) purrr::map_dfr(.x = site, ~ firearea::retrieve_usgs_chem(station_id = .x$usgs_site))}()
```

Because of the large number of columns in the water chemistry output and because we want to preserve all of these data, easiest is to use the `DBI::dbWriteTable` function to create the table; we use the append parameter to add additional data to the table once it is created.

```{r}
#| eval: FALSE

DBI::dbWriteTable(
  conn      = pg,
  name      = c("firearea", "water_chem"),
  value     = usgs_water_chem,
  overwrite = FALSE,
  append    = TRUE,
  row.names = FALSE
)
```

Upon initial creation, modify fields and generate an index before adding additional data.

A note about timestamps:
- `ActivityStartTime.Time` and `ActivityEndTime.Time` are local time where local is the time zone of the USGS site
- `ActivityStartDateTime` and `ActivityEndDateTime` are UTC
- Postgres assumes UTC for TYPE TIME WITHOUT TIME ZONE so date and time operations should not be performed on the fields where the time zone is local (because Postgres is assuming UTC).

```{r}
#| eval: FALSE

# run once

# DBI::dbExecute(
#   conn      = pg,
#   statement = '
#     ALTER TABLE firearea.water_chem 
#     ALTER COLUMN "ActivityStartTime.Time" TYPE TIME WITHOUT TIME ZONE USING "ActivityStartTime.Time"::TIME WITHOUT TIME ZONE,
#     ALTER COLUMN "ActivityEndTime.Time"   TYPE TIME WITHOUT TIME ZONE USING "ActivityEndTime.Time"::TIME WITHOUT TIME ZONE,
#     ALTER COLUMN "ActivityStartDateTime"  TYPE TIMESTAMP WITHOUT TIME ZONE USING "ActivityStartDateTime"::TIMESTAMP WITHOUT TIME ZONE,
#     ALTER COLUMN "ActivityEndDateTime"    TYPE TIMESTAMP WITHOUT TIME ZONE USING "ActivityEndDateTime"::TIMESTAMP WITHOUT TIME ZONE
#   ;
#     CREATE INDEX idx_water_chem_site_analyte ON firearea.water_chem (usgs_site, "USGSPCode")
#   ;
#   '
# )
```


# daily discharge --------------------------------------------------------------

We use the available water chemistry data to inform harvesting daily discharge data for a given site where, for each site, we pull discharge data one month prior to the earliest record for any of our analytes of interest. 

There are a couple of approaches to identifying the locations and temporal extents for which to harvest discharge data. We can address this iteratively, that is each time that we get `usgs_water_chem` for a given state, we can get the locations and corresponding earliest water chemistry data to pull from those data from that state, sensu:

Note in the code blocks below is a rare case where we need to load a library as it is not clear how to namespace the call to `{r}%m-%` from lubridate; using a base-R approach (e.g., `{r}as.character(chem_earliest - months(1)`) generates unexpected NAs.

```{r}
#| eval: FALSE

library(lubridate)

water_chem_timing <- usgs_water_chem |>
  dplyr::group_by(usgs_site) |>
  dplyr::summarise(chem_earliest = min(ActivityStartDate)) |>
  dplyr::mutate(
    prior_month = chem_earliest %m-% base::months(1),
    usgs_site   = stringr::str_extract(usgs_site, "(?<=-)[0-9]+$")
  )

```

Alternatively, we can pull the sites (and earliest water chemistry of interest) for all or a subset of sites for which water chemistry data have been loaded into the database as in below. 

```{r}
#| eval: FALSE

water_chem_timing <- DBI::dbGetQuery(
  conn      = pg,
  statement = '
  SELECT
    usgs_site,
    MIN("ActivityStartDate")
  FROM
    firearea.water_chem
  GROUP BY
    usgs_site
  ;
  '
)

library(lubridate)

water_chem_timing <- water_chem_timing |> 
  dplyr::rename(chem_earliest = min) |> 
  dplyr::mutate(
    prior_month = chem_earliest %m-% base::months(1),
    usgs_site   = stringr::str_extract(usgs_site, "(?<=-)[0-9]+$")
  )

```

With the water chemistry data for each site, we can then use the earliest record (minus one month) to harvest the corresponding discharge data.

```{r}
#| eval: FALSE

retrieve_usgs_discharge_possibly <- purrr::possibly(
  .f        = firearea::retrieve_usgs_discharge,
  otherwise = NULL
)

discharge_daily <- split(
  x = water_chem_timing,
  f = water_chem_timing$usgs_site
  ) |>
{\(site) purrr::map_dfr(.x = site, ~ retrieve_usgs_discharge_possibly(station_id = .x$usgs_site, start_date = .x$prior_month, daily = TRUE))}()

discharge_daily_expected_cols <- c("agency_cd", "site_no", "dateTime", "tz_cd", "Date", "Flow", "Flow_cd", "usgs_site")

discharge_daily <- discharge_daily |>
  dplyr::mutate(usgs_site = paste0("USGS-", site_no)) |>      # construct usgs_site from site_no
  dplyr::select(dplyr::any_of(discharge_daily_expected_cols)) # constrain to expected columns
```

As with water chemistry, easiest is to use the `DBI::dbWriteTable` function to create the table; we use the append parameter to add additional data to the table once it is created.

```{r}
#| eval: FALSE
DBI::dbWriteTable(
  conn      = pg,
  name      = c("firearea", "discharge_daily"),
  value     = discharge_daily,
  overwrite = FALSE,
  append    = TRUE,
  row.names = FALSE
)
```

Upon initial creation, modify fields and generate an index before adding additional data.

```{r}
#| eval: FALSE

# run once

# DBI::dbExecute(
#   conn      = pg,
#   statement = '
#   ALTER TABLE firearea.discharge_daily ALTER COLUMN "dateTime" TYPE DATE ;
#   CREATE INDEX idx_discharge_daily_usgs_site ON firearea.discharge_daily (usgs_site) ;
#   '
# )
```


# delineated catchments --------------------------------------------------------

Whereas the firearea vignette outlined an iterative state-by-state approach to identifying sites, harvesting chemistry, discharge, and delineating catchments, we can just as well identify the sites to delineate from discharge_daily (or water_chem) data in the R environment...

```{r}
#| eval: FALSE

stream_sites_candidates <- tibble::tibble(usgs_site = unique(discharge_daily$site_no))
```

...or pulled from the database...


```{r}
#| eval: FALSE

stream_sites_candidates <- DBI::dbGetQuery(
  conn      = pg,
  statement = '
  SELECT
    DISTINCT(usgs_site)
  FROM
    firearea.discharge_daily
  ;
  '
)

stream_sites_candidates$usgs_site <- gsub("USGS-", "", stream_sites_candidates$usgs_site)
```


Whereas the firearea vignettes focus on a workflow of delineating catchments generally, the workflow here is tailored specifically to developing the wilfire database and, as such, draws on firearea::write_usgs_catchment_delineation to delineate the catchment for a/each USGS site and load it in to the wildfire.firearea.catchments table.

```{r}
#| eval: FALSE

wucd_possibly <- purrr::possibly(
  .f        = firearea::write_usgs_catchment_delineation,
  otherwise = NULL
)

purrr::walk(stream_sites_candidates$usgs_site, ~ wucd_possibly(usgs_site = .x))
```

```{r}
#| eval: FALSE

# run once

DBI::dbExecute(
  conn      = pg,
  statement = "ALTER TABLE firearea.catchments ADD PRIMARY KEY (usgs_site) ;"
)

DBI::dbExecute(
  conn      = pg,
  statement = "CREATE INDEX catchments_geometry_idx ON firearea.catchments USING GIST (geometry) ;"
)

DBI::dbExecute(
  conn      = pg,
  statement = "
  ALTER TABLE firearea.catchments
    ALTER COLUMN geometry
    TYPE geometry(geometry, 4326)
    USING ST_SetSRID(geometry, 4326)
  ;
  "
)
```


# MTBS -------------------------------------------------------------------------

These MTBS data were downloaded 2023-12-08. Here we are keeping the source data's 4269 CRS.

```sh
shp2pgsql -D -I -s 4269 mtbs_perims_DD.shp firearea.mtbs_fire_perimeters | psql -h localhost -U _user_ -d wildfire
```

```sql
CREATE UNIQUE INDEX unique_mtbs_fire_perimeters_event_id ON firearea.mtbs_fire_perimeters (event_id) ;

UPDATE firearea.mtbs_fire_perimeters
SET geom = st_multi(st_collectionextract(st_makevalid(geom), 3))
WHERE mtbs_fire_perimeters.event_id IN (
  SELECT
    event_id
  FROM
    firearea.mtbs_fire_perimeters
  WHERE
    NOT ST_IsValid(geom)
)
;
```

# ecoregions -------------------------------------------------------------------

Add [ecoregions](https://github.com/lter/lter-sparc-fire-arid-streams/blob/main/site_selection/aridity_ecoregions.R) that define the study area.

```sh
ogr2ogr -f PostgreSQL PG:"host=localhost user=_user_ dbname=wildfire_dev active_schema=firearea" aridland_ecoregions.geojson -nln ecoregions
```

Collapse the collection of ecoregion polygons into a single polygon and filter catchments that intersect and have at least 75% overlap within that area:

```sql

DROP TABLE IF EXISTS firearea.ecoregion_catchments ;

WITH ecoregion AS (
  SELECT
    st_union(wkb_geometry) AS geometry
  FROM firearea.ecoregions
)
SELECT
    intersection.usgs_site,
    intersection.ecoregion_pct,
    intersection.geometry
INTO firearea.ecoregion_catchments
FROM (
    SELECT
        catchments.usgs_site,
        ROUND((ST_Area(ST_Intersection(catchments.geometry, ecoregion.geometry)) / ST_Area(catchments.geometry) * 100)::numeric, 1) AS ecoregion_pct,
        catchments.geometry
    FROM firearea.catchments, ecoregion
    WHERE st_intersects(catchments.geometry, ecoregion.geometry)
) AS intersection
WHERE intersection.ecoregion_pct >= 75
;

ALTER TABLE firearea.ecoregion_catchments ADD PRIMARY KEY (usgs_site) ;
```


# fires_catchments -------------------------------------------------------------

Recreate this table as needed when new sites sites are added and/or if firearea.ecoregion_catchments changes.

```sql
DROP TABLE IF EXISTS firearea.fires_catchments ;

WITH RECURSIVE
perimeters_transform AS (
    SELECT
        gid,
        event_id,
        ig_date,
        ST_Transform(geom, 4326) AS geom
    FROM
        firearea.mtbs_fire_perimeters
),
fires_catchments AS (
  SELECT
    ecoregion_catchments.usgs_site,
    -- catchments.area_km2,
    perimeters_transform.event_id,
    perimeters_transform.ig_date,
    ST_Intersection(perimeters_transform.geom, ecoregion_catchments.geometry) AS geometry
  FROM perimeters_transform
  INNER JOIN firearea.ecoregion_catchments ON ST_Intersects(perimeters_transform.geom, ecoregion_catchments.geometry)
  WHERE
    ST_isvalid(perimeters_transform.geom) = 'TRUE' AND
    ST_isvalid(ecoregion_catchments.geometry) = 'TRUE'
)
SELECT
  DISTINCT fires_catchments.*
INTO firearea.fires_catchments
FROM fires_catchments
ORDER BY 
  fires_catchments.usgs_site,
  fires_catchments.event_id
;

ALTER TABLE firearea.fires_catchments
  ALTER COLUMN geometry
  TYPE geometry(geometry, 4326)
  USING ST_SetSRID(geometry, 4326)
;

CREATE UNIQUE INDEX idx_fires_catchments_usgs_site_event_id ON firearea.fires_catchments (usgs_site, event_id) ;
```

# pour points ------------------------------------------------------------------

As with catchment delineations, the workflow here is tailored specifically to developing the wilfire database and, as such, draws on firearea::write_usgs_pour_point to identify the catchment for a/each USGS site and load it in to the wildfire.firearea.pour_points table.

This workflow is specific to the four-corner states, Nevada, and California that were part of the initial database development. For additional sites, the workflow was modified such that pour points are written in the same step with catchments.


```{r}
#| eval: FALSE

stream_sites_candidates <- DBI::dbGetQuery(
  conn      = pg,
  statement = "
  SELECT
    DISTINCT(usgs_site)
  FROM
    firearea.catchments
  ;
  "
)

stream_sites_candidates$usgs_site <- gsub("USGS-", "", stream_sites_candidates$usgs_site)
```

```{r}
#| eval: FALSE

wupp_possibly <- purrr::possibly(
  .f        = firearea::write_usgs_pour_point,
  otherwise = NULL
)

purrr::walk(stream_sites_candidates[["usgs_site"]], ~ wupp_possibly(usgs_site = .x))
```

Upon initial creation, modify fields and generate an index.

```{r}
#| eval: FALSE

DBI::dbExecute(
  conn      = pg,
  statement = "ALTER TABLE firearea.pour_points ADD PRIMARY KEY (usgs_site) ;"
)

DBI::dbExecute(
  conn      = pg,
  statement = "CREATE INDEX pour_points_geometry_idx ON firearea.pour_points USING GIST (geometry) ;"
)

DBI::dbExecute(
  conn      = pg,
  statement = "
  ALTER TABLE firearea.pour_points
  ALTER COLUMN geometry
  TYPE geometry(geometry, 4326)
  USING ST_SetSRID(geometry, 4326)
  ;
  "
)
```

# flowlines --------------------------------------------------------------------

Note that this table does not have a primary key.

```{sql}
#| eval: FALSE

CREATE TABLE firearea.flowlines (
  nhdplus_comid text,
  geometry public.geometry,
  usgs_site text
);

ALTER TABLE ONLY firearea.flowlines
ADD CONSTRAINT flowlines_usgs_site_nhdplus_comid_unique UNIQUE (usgs_site, nhdplus_comid)
;


ALTER TABLE firearea.flowlines
  ALTER COLUMN geometry
  TYPE geometry(geometry, 4326)
  USING ST_SetSRID(geometry, 4326)
;

)
```


```{r}

ecoregion_catchment_sites <- DBI::dbGetQuery(
  conn      = pg,
  statement = "
  SELECT usgs_site
  FROM firearea.ecoregion_catchments
  ;
  "
) |>
  dplyr::mutate(
    site_id = gsub(
      pattern     = "usgs-",
      replacement = "",
      x           = usgs_site,
      ignore.case = TRUE
    )
  )

write_flowlines_safely <- purrr::safely(firearea::write_usgs_flowlines)

# add progress tracking
total_sites <- length(ecoregion_catchment_sites$site_id)
message("Processing ", total_sites, " sites with rate limiting...")

results <- purrr::map(
  .x = ecoregion_catchment_sites$site_id,
  .f = ~ {

    # progress message
    current_index <- which(ecoregion_catchment_sites$site_id == .x)
    if (current_index %% 10 == 0 || current_index == 1) {
      message("Processing site ", current_index, " of ", total_sites, ": ", .x)
    }
    
    # call the function
    result <- write_flowlines_safely(usgs_site = .x)
    
    # rate limiting delay (except for the last iteration)
    if (current_index < total_sites) {
      Sys.sleep(60)  # delay
    }
    
    return(result)
  }
)

# flowlines successes and errors

successes      <- purrr::map_lgl(results, ~ is.null(.x$error))
flowline_sites <- ecoregion_catchment_sites$usgs_site

result_df  <- tibble::tibble(
  usgs_site = flowline_sites,
  success   = successes,
  error     = purrr::map_chr(
    results,
    ~ if (!is.null(.x$error)) as.character(.x$error) else NA_character_
  )
)

```
